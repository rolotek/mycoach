---
phase: 02-coaching-memory
plan: "01"
type: execute
wave: 1
depends_on: []
files_modified:
  - apps/server/src/db/schema.ts
  - apps/server/src/memory/embeddings.ts
  - apps/server/src/memory/chunker.ts
  - apps/server/package.json
  - apps/server/scripts/enable-pgvector.ts
autonomous: true

must_haves:
  truths:
    - "conversations, memories, documents, and userFacts tables exist in the database"
    - "System can perform semantic similarity search on memories and documents"
    - "Text can be embedded into 1536-dimension vectors using OpenAI text-embedding-3-small"
    - "Text can be chunked into overlapping segments suitable for embedding"
  artifacts:
    - path: "apps/server/src/db/schema.ts"
      provides: "conversations, memories, documents, userFacts table definitions with vector columns and HNSW indexes"
      contains: "conversations|memories|documents|userFacts"
    - path: "apps/server/src/memory/embeddings.ts"
      provides: "embed and embedMany wrappers using OpenAI text-embedding-3-small"
      exports: ["embedText", "embedTexts"]
    - path: "apps/server/src/memory/chunker.ts"
      provides: "Recursive character text splitter with overlap"
      exports: ["chunkText"]
    - path: "apps/server/scripts/enable-pgvector.ts"
      provides: "Script to enable pgvector extension before schema push"
  key_links:
    - from: "apps/server/src/memory/embeddings.ts"
      to: "@ai-sdk/openai"
      via: "openai.embedding('text-embedding-3-small')"
      pattern: "text-embedding-3-small"
    - from: "apps/server/src/db/schema.ts"
      to: "drizzle-orm/pg-core"
      via: "vector column type"
      pattern: "vector.*1536"
---

<objective>
Extend the database schema with coaching & memory tables (conversations, memories, documents, userFacts) including pgvector columns and HNSW indexes, and create embedding + text chunking utilities.

Purpose: Establish the data foundation for all Phase 2 features -- streaming chat persistence, RAG retrieval, document knowledge, and user fact storage.
Output: Extended schema.ts, embedding utilities, chunking utility, pgvector enable script.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-coaching-memory/02-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
@.planning/phases/01-foundation/01-03-SUMMARY.md

@apps/server/src/db/schema.ts
@apps/server/src/db/index.ts
@apps/server/package.json
@apps/server/src/llm/registry.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend database schema with coaching & memory tables</name>
  <files>apps/server/src/db/schema.ts</files>
  <action>
Add four new tables to the existing schema.ts file (keep all existing tables intact):

1. **conversations** table:
   - `id`: uuid, primaryKey, defaultRandom
   - `userId`: text, notNull, references user.id with onDelete cascade
   - `title`: text (nullable, auto-generated later)
   - `mode`: text, default "auto" (values: "auto" | "coaching" | "task")
   - `messages`: jsonb, default [] (stores UIMessage[] from AI SDK)
   - `createdAt`: timestamp, defaultNow, notNull
   - `updatedAt`: timestamp, defaultNow, notNull
   - Index on userId

2. **memories** table:
   - `id`: uuid, primaryKey, defaultRandom
   - `userId`: text, notNull, references user.id with onDelete cascade
   - `conversationId`: uuid, nullable, references conversations.id with onDelete set null
   - `content`: text, notNull
   - `embedding`: vector('embedding', { dimensions: 1536 }) — import vector from drizzle-orm/pg-core
   - `type`: text, notNull (values: "conversation_chunk" | "document_chunk" | "summary")
   - `metadata`: jsonb, default {}
   - `createdAt`: timestamp, defaultNow, notNull
   - Index on userId
   - HNSW index on embedding using vector_cosine_ops

3. **documents** table:
   - `id`: uuid, primaryKey, defaultRandom
   - `userId`: text, notNull, references user.id with onDelete cascade
   - `filename`: text, notNull
   - `mimeType`: text, notNull
   - `content`: text (nullable — extracted text)
   - `size`: integer, notNull
   - `status`: text, default "processing" (values: "processing" | "ready" | "error")
   - `createdAt`: timestamp, defaultNow, notNull
   - Index on userId

4. **userFacts** table:
   - `id`: uuid, primaryKey, defaultRandom
   - `userId`: text, notNull, references user.id with onDelete cascade
   - `category`: text, notNull (values: "goal" | "preference" | "context" | "relationship" | "work" | "personal")
   - `fact`: text, notNull
   - `confidence`: real, default 0.8 — import `real` from drizzle-orm/pg-core
   - `source`: text (nullable, values: "conversation" | "document" | "manual")
   - `sourceId`: uuid (nullable — conversation or document ID)
   - `embedding`: vector('embedding', { dimensions: 1536 })
   - `createdAt`: timestamp, defaultNow, notNull
   - `updatedAt`: timestamp, defaultNow, notNull
   - Index on userId
   - HNSW index on embedding using vector_cosine_ops

Add Drizzle relations:
- user has many conversations, documents, userFacts
- conversation belongs to user
- Update existing userRelations to include conversations, documents, userFacts

Import `vector`, `real`, and `integer` from drizzle-orm/pg-core (integer is already available, add vector and real).

For the HNSW index syntax with Drizzle, use:
```typescript
index("memories_embedding_idx").using("hnsw", table.embedding.op("vector_cosine_ops"))
```
  </action>
  <verify>Run `cd apps/server && npx tsc --noEmit` — no type errors in schema.ts. Verify the file exports conversations, memories, documents, userFacts.</verify>
  <done>All four tables defined in schema.ts with correct column types, references, indexes (including HNSW vector indexes), and relations. TypeScript compiles without errors.</done>
</task>

<task type="auto">
  <name>Task 2: Create embedding utilities, text chunker, and pgvector enable script</name>
  <files>
    apps/server/src/memory/embeddings.ts
    apps/server/src/memory/chunker.ts
    apps/server/scripts/enable-pgvector.ts
    apps/server/package.json
  </files>
  <action>
**1. Create `apps/server/src/memory/embeddings.ts`:**

```typescript
import { embed, embedMany } from "ai";
import { openai } from "@ai-sdk/openai";

const embeddingModel = openai.embedding("text-embedding-3-small");

export async function embedText(text: string): Promise<number[]> {
  const { embedding } = await embed({ model: embeddingModel, value: text });
  return embedding;
}

export async function embedTexts(texts: string[]): Promise<number[][]> {
  const { embeddings } = await embedMany({ model: embeddingModel, values: texts });
  return embeddings;
}
```

Note: `openai` is already available via `@ai-sdk/openai` which is in the server's package.json. The `createOpenAI()` call in registry.ts creates the provider. Here, import the default `openai` export from `@ai-sdk/openai` directly for embedding access (the registry is for language models, embeddings use the provider directly).

**2. Create `apps/server/src/memory/chunker.ts`:**

Implement a recursive character text splitter with configurable chunk size and overlap:

```typescript
export function chunkText(
  text: string,
  maxChunkSize: number = 500,
  overlap: number = 50
): string[] {
  if (!text || text.trim().length === 0) return [];
  if (text.length <= maxChunkSize) return [text.trim()];

  const separators = ["\n\n", "\n", ". ", " "];

  function splitWithSeparator(text: string, sepIndex: number): string[] {
    if (text.length <= maxChunkSize) return [text];
    if (sepIndex >= separators.length) {
      // Hard split if no separator works
      const chunks: string[] = [];
      for (let i = 0; i < text.length; i += maxChunkSize - overlap) {
        chunks.push(text.slice(i, i + maxChunkSize));
      }
      return chunks;
    }

    const sep = separators[sepIndex];
    const parts = text.split(sep);
    const result: string[] = [];
    let current = "";

    for (const part of parts) {
      const candidate = current ? current + sep + part : part;
      if (candidate.length > maxChunkSize && current) {
        result.push(current.trim());
        current = current.slice(-overlap) + sep + part;
      } else {
        current = candidate;
      }
    }
    if (current.trim()) result.push(current.trim());

    // Recursively split any chunks that are still too large
    return result.flatMap((chunk) =>
      chunk.length > maxChunkSize
        ? splitWithSeparator(chunk, sepIndex + 1)
        : [chunk]
    );
  }

  return splitWithSeparator(text, 0).filter((c) => c.length > 0);
}
```

**3. Create `apps/server/scripts/enable-pgvector.ts`:**

A script that enables the pgvector extension before schema push. This is necessary because Drizzle does not automatically create the extension.

```typescript
import "dotenv/config";
import pg from "pg";

async function enablePgvector() {
  const client = new pg.Client({ connectionString: process.env.DATABASE_URL });
  await client.connect();
  try {
    await client.query("CREATE EXTENSION IF NOT EXISTS vector;");
    console.log("pgvector extension enabled");
  } finally {
    await client.end();
  }
}

enablePgvector().catch((err) => {
  console.error("Failed to enable pgvector:", err);
  process.exit(1);
});
```

Note: Use dotenv/config for env loading (dotenv is already a dependency). Import pg using `import pg from "pg"` since the project uses ESM-compatible imports.

**4. Update `apps/server/package.json`:**

Add a script to enable pgvector before db push:
```json
"db:pgvector": "tsx scripts/enable-pgvector.ts",
"db:setup": "tsx scripts/enable-pgvector.ts && node scripts/db-push.js"
```

Do NOT modify existing scripts — only add new ones.
  </action>
  <verify>
1. `cd apps/server && npx tsc --noEmit` — no type errors
2. `cd apps/server && npx tsx scripts/enable-pgvector.ts` — prints "pgvector extension enabled" (requires DATABASE_URL set and Postgres running with pgvector installed)
3. Verify embeddings.ts and chunker.ts export their functions
  </verify>
  <done>Embedding utilities (embedText, embedTexts) use OpenAI text-embedding-3-small via AI SDK. Text chunker splits text recursively with overlap. pgvector enable script works. All TypeScript compiles.</done>
</task>

</tasks>

<verification>
1. `cd apps/server && npx tsc --noEmit` passes
2. Schema exports conversations, memories, documents, userFacts with correct types
3. Vector columns use 1536 dimensions
4. HNSW indexes defined on embedding columns
5. embedText/embedTexts functions properly typed
6. chunkText produces reasonable chunks for sample text
7. enable-pgvector script runs without error against local Postgres
</verification>

<success_criteria>
- Four new tables (conversations, memories, documents, userFacts) defined in schema.ts with proper columns, references, and indexes
- pgvector extension can be enabled via script
- Embedding functions wrap AI SDK embed/embedMany with OpenAI text-embedding-3-small
- Text chunker splits text into overlapping chunks suitable for embedding
- All TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/02-coaching-memory/02-01-SUMMARY.md`
</output>

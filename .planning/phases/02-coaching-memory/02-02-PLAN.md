---
phase: 02-coaching-memory
plan: "02"
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - apps/server/src/coaching/chat-route.ts
  - apps/server/src/coaching/system-prompt.ts
  - apps/server/src/coaching/mode-detector.ts
  - apps/server/src/coaching/structured-outputs.ts
  - apps/server/src/memory/retriever.ts
  - apps/server/src/memory/fact-extractor.ts
  - apps/server/src/index.ts
autonomous: true

must_haves:
  truths:
    - "POST /api/chat returns a streaming SSE response compatible with useChat"
    - "Chat responses reference relevant prior context from memories table"
    - "System auto-detects coaching vs task mode based on user message"
    - "User can override mode via a mode field in the request body"
    - "Facts are extracted from conversations and stored in userFacts table after each response"
    - "Conversation messages are persisted to the conversations table"
  artifacts:
    - path: "apps/server/src/coaching/chat-route.ts"
      provides: "Hono POST /api/chat with streamText + toUIMessageStreamResponse"
      exports: ["chatApp"]
    - path: "apps/server/src/coaching/system-prompt.ts"
      provides: "System prompt builder with coaching persona and RAG context injection"
      exports: ["buildSystemPrompt"]
    - path: "apps/server/src/coaching/mode-detector.ts"
      provides: "Mode detection logic for coaching vs task mode"
      exports: ["detectMode"]
    - path: "apps/server/src/coaching/structured-outputs.ts"
      provides: "Zod schemas for structured outputs (action items, decision frameworks, summaries)"
      exports: ["actionItemsSchema", "decisionFrameworkSchema", "summarySchema"]
    - path: "apps/server/src/memory/retriever.ts"
      provides: "Semantic search over memories table using cosine distance"
      exports: ["retrieveContext"]
    - path: "apps/server/src/memory/fact-extractor.ts"
      provides: "Background fact extraction from conversation messages"
      exports: ["extractFacts"]
  key_links:
    - from: "apps/server/src/coaching/chat-route.ts"
      to: "apps/server/src/memory/retriever.ts"
      via: "retrieveContext call before streamText"
      pattern: "retrieveContext"
    - from: "apps/server/src/coaching/chat-route.ts"
      to: "apps/server/src/memory/fact-extractor.ts"
      via: "extractFacts in onFinish callback"
      pattern: "onFinish.*extractFacts"
    - from: "apps/server/src/coaching/chat-route.ts"
      to: "apps/server/src/coaching/system-prompt.ts"
      via: "buildSystemPrompt with RAG context"
      pattern: "buildSystemPrompt"
    - from: "apps/server/src/coaching/chat-route.ts"
      to: "apps/server/src/coaching/structured-outputs.ts"
      via: "Zod schemas registered as streamText tools in task mode"
      pattern: "actionItemsSchema|decisionFrameworkSchema|summarySchema"
    - from: "apps/server/src/index.ts"
      to: "apps/server/src/coaching/chat-route.ts"
      via: "app.route mounting"
      pattern: "chatApp"
---

<objective>
Build the streaming chat endpoint with RAG context retrieval, mode detection, background fact extraction, and conversation persistence.

Purpose: This is the core intelligence layer -- the coaching conversation engine that makes the AI contextual, persistent, and modal. It handles COACH-01 (streaming chat), COACH-02 (references prior context), COACH-03 (structured outputs via AI SDK tools in task mode), COACH-04 (auto-detect mode), COACH-05 (manual mode override), and MEM-01 (fact extraction).
Output: Chat route, system prompt builder, mode detector, structured output schemas, RAG retriever, fact extractor.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-coaching-memory/02-RESEARCH.md
@.planning/phases/02-coaching-memory/02-01-SUMMARY.md

@apps/server/src/index.ts
@apps/server/src/db/schema.ts
@apps/server/src/db/index.ts
@apps/server/src/llm/providers.ts
@apps/server/src/llm/registry.ts
@apps/server/src/middleware/auth.ts
@apps/server/src/auth.ts
@apps/server/src/memory/embeddings.ts
@apps/server/src/memory/chunker.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create streaming chat route with system prompt and mode detection</name>
  <files>
    apps/server/src/coaching/chat-route.ts
    apps/server/src/coaching/system-prompt.ts
    apps/server/src/coaching/mode-detector.ts
    apps/server/src/coaching/structured-outputs.ts
    apps/server/src/index.ts
  </files>
  <action>
**1. Create `apps/server/src/coaching/structured-outputs.ts`:**

Define Zod schemas for structured outputs that the coach can produce in task mode:

- `actionItemsSchema`: Array of { title, description, priority (high/medium/low), dueDate? }, plus summary
- `decisionFrameworkSchema`: { question, options: Array of { name, pros, cons, riskLevel }, recommendation, reasoning }
- `summarySchema`: { title, keyPoints: string[], decisions: string[], nextSteps: string[] }

Export all three schemas.

**2. Create `apps/server/src/coaching/mode-detector.ts`:**

Export a `detectMode` function that takes the user's latest message text and the conversation's current mode setting:

```typescript
export type ConversationMode = "auto" | "coaching" | "task";
export type EffectiveMode = "coaching" | "task";

export function detectMode(
  userMessage: string,
  modeOverride: ConversationMode
): EffectiveMode {
  // If user has manually set mode, honor it
  if (modeOverride === "coaching") return "coaching";
  if (modeOverride === "task") return "task";

  // Auto-detect: check for task-mode signals
  const taskSignals = [
    /action items/i,
    /summarize/i,
    /summary/i,
    /decision framework/i,
    /pros and cons/i,
    /create a plan/i,
    /list.*steps/i,
    /one[- ]pager/i,
    /draft.*email/i,
    /write.*memo/i,
    /prepare.*brief/i,
  ];

  return taskSignals.some((r) => r.test(userMessage)) ? "task" : "coaching";
}
```

**3. Create `apps/server/src/coaching/system-prompt.ts`:**

Export a `buildSystemPrompt` function that takes:
- `relevantContext`: Array of { content: string, similarity: number }
- `userFacts`: Array of { category: string, fact: string }
- `mode`: EffectiveMode

Returns a system prompt string that includes:
- The coaching persona (executive and life coach, chief of staff)
- Mode instructions (coaching = reflective dialogue, task = structured output)
- Injected RAG context section (memories + document chunks)
- Injected user facts section
- Instructions to reference context naturally, not dump it
- When in task mode, instruct the model to produce structured output (action items, decision frameworks, summaries) as appropriate

The prompt should be concise but comprehensive. Include these sections:
```
## Identity
You are an executive coach and chief of staff...

## Mode: {mode}
{coaching instructions OR task instructions}

## What You Know About This User
{user facts by category, or "No facts recorded yet"}

## Relevant Context
{RAG retrieved content, or "No prior context available"}

## Guidelines
- Reference context naturally
- Ask clarifying questions
- Be direct but empathetic
- In task mode, produce structured markdown output
```

**4. Create `apps/server/src/coaching/chat-route.ts`:**

Create a Hono sub-app with POST /api/chat endpoint:

```typescript
import { Hono } from "hono";
import { streamText, convertToModelMessages, UIMessage } from "ai";
import { eq, and, desc } from "drizzle-orm";
import { db } from "../db";
import { conversations, userFacts } from "../db/schema";
import { getModel } from "../llm/providers";
import { buildSystemPrompt } from "./system-prompt";
import { detectMode, ConversationMode } from "./mode-detector";
import { actionItemsSchema, decisionFrameworkSchema, summarySchema } from "./structured-outputs";
import { retrieveContext } from "../memory/retriever";
import { extractFacts } from "../memory/fact-extractor";

// Type the Hono app with the same Variables as the main app
const chatApp = new Hono<{
  Variables: {
    user: { id: string; name: string; email: string } | null;
    session: unknown;
  };
}>();

chatApp.post("/api/chat", async (c) => {
  const user = c.get("user");
  if (!user) return c.json({ error: "Unauthorized" }, 401);

  const body = await c.req.json<{
    messages: UIMessage[];
    chatId?: string;
    mode?: ConversationMode;
  }>();

  const { messages, mode: modeOverride = "auto" } = body;
  let chatId = body.chatId;

  // Get or create conversation
  if (!chatId) {
    const [conv] = await db
      .insert(conversations)
      .values({ userId: user.id, mode: modeOverride })
      .returning();
    chatId = conv.id;
  }

  // Get the last user message for mode detection and retrieval
  const lastUserMsg = [...messages].reverse().find((m) => m.role === "user");
  const queryText = lastUserMsg?.content?.toString() ?? "";

  // Detect mode
  const effectiveMode = detectMode(queryText, modeOverride);

  // Retrieve relevant context
  const relevantContext = await retrieveContext(user.id, queryText);

  // Retrieve user facts
  const facts = await db
    .select({ category: userFacts.category, fact: userFacts.fact })
    .from(userFacts)
    .where(eq(userFacts.userId, user.id))
    .orderBy(desc(userFacts.confidence))
    .limit(20);

  // Get user's preferred model from settings, fallback to default
  // For now, use a sensible default. The model selection will come from user settings.
  const modelId = "anthropic:claude-sonnet-4-20250514";

  const systemPrompt = buildSystemPrompt(relevantContext, facts, effectiveMode);

  // When in task mode, provide structured output tools so the model can
  // produce action items, decision frameworks, or summaries as structured data.
  // The model decides which tool to call based on the user's request.
  // In coaching mode, no tools are provided — the model responds conversationally.
  const taskTools = effectiveMode === "task" ? {
    tools: {
      createActionItems: {
        description: "Create a structured list of action items with priorities and due dates",
        parameters: actionItemsSchema,
      },
      createDecisionFramework: {
        description: "Create a structured decision framework with options, pros/cons, and recommendation",
        parameters: decisionFrameworkSchema,
      },
      createSummary: {
        description: "Create a structured summary with key points, decisions, and next steps",
        parameters: summarySchema,
      },
    },
  } : {};

  const result = streamText({
    model: getModel(modelId),
    system: systemPrompt,
    messages: convertToModelMessages(messages),
    ...taskTools,
  });

  return result.toUIMessageStreamResponse({
    sendReasoning: false,
    headers: { "X-Chat-Id": chatId },
    onFinish: async ({ response }) => {
      // Persist messages to conversation
      const allMessages = [...messages];
      // Add assistant response from the finish event
      if (response.messages.length > 0) {
        // response.messages contains the assistant response in ModelMessage format
        // Store the full UIMessage array for persistence
      }

      await db
        .update(conversations)
        .set({
          messages: JSON.stringify(messages),
          updatedAt: new Date(),
          title: messages.length <= 2 ? queryText.slice(0, 100) : undefined,
        })
        .where(
          and(
            eq(conversations.id, chatId!),
            eq(conversations.userId, user.id)
          )
        );

      // Background fact extraction (fire and forget)
      extractFacts(chatId!, user.id, messages).catch((err) =>
        console.error("Fact extraction failed:", err)
      );
    },
  });
});

export { chatApp };
```

Important implementation notes:
- The chat route does NOT go through tRPC (research: tRPC doesn't support SSE UI Message Stream Protocol)
- Auth comes from the session middleware already applied in index.ts
- `convertToModelMessages` from "ai" converts UIMessage[] to the format streamText expects
- `toUIMessageStreamResponse()` produces the correct SSE format for useChat
- Return the chatId in a response header so the client can track it
- The onFinish callback persists messages and triggers background fact extraction
- Use `lastUserMsg.content?.toString()` because UIMessage content can be string or parts
- **Structured outputs (COACH-03):** When effectiveMode is "task", the three Zod schemas from structured-outputs.ts are registered as AI SDK tools on the streamText call. The model can invoke `createActionItems`, `createDecisionFramework`, or `createSummary` to produce structured data. The client receives tool call results via the stream and renders them (see plan 02-04). In coaching mode, no tools are provided, so the model responds conversationally only.

**5. Update `apps/server/src/index.ts`:**

Add CORS for `/api/chat` and `/api/documents/*`, and mount the chatApp:

- Add `app.use("/api/chat", cors(corsOptions))` after the existing CORS lines
- Add `app.use("/api/documents/*", cors(corsOptions))` after that
- Import `chatApp` from `./coaching/chat-route`
- Mount with `app.route("", chatApp)` AFTER the session middleware but BEFORE the health check
- Ensure the session middleware runs before the chat route (it already applies to "*")
  </action>
  <verify>
1. `cd apps/server && npx tsc --noEmit` — no type errors
2. Verify chat-route.ts exports chatApp
3. Verify index.ts imports and mounts chatApp
4. Verify CORS is configured for /api/chat
  </verify>
  <done>POST /api/chat returns streaming SSE via toUIMessageStreamResponse. System prompt includes coaching persona, mode instructions, RAG context, and user facts. Mode detection works for auto/coaching/task. In task mode, structured output Zod schemas are registered as tools so the model can produce action items, decision frameworks, and summaries. Chat is mounted in index.ts with proper CORS.</done>
</task>

<task type="auto">
  <name>Task 2: Create semantic retriever and background fact extractor</name>
  <files>
    apps/server/src/memory/retriever.ts
    apps/server/src/memory/fact-extractor.ts
  </files>
  <action>
**1. Create `apps/server/src/memory/retriever.ts`:**

Semantic search over the memories table using pgvector cosine distance:

```typescript
import { embed } from "ai";
import { openai } from "@ai-sdk/openai";
import { cosineDistance, desc, gt, sql, eq, and } from "drizzle-orm";
import { db } from "../db";
import { memories } from "../db/schema";

const embeddingModel = openai.embedding("text-embedding-3-small");

export async function retrieveContext(
  userId: string,
  query: string,
  limit: number = 10,
  minSimilarity: number = 0.3
): Promise<Array<{ content: string; similarity: number; type: string }>> {
  if (!query.trim()) return [];

  try {
    const { embedding } = await embed({
      model: embeddingModel,
      value: query,
    });

    const similarity = sql<number>`1 - (${cosineDistance(memories.embedding, embedding)})`;

    const results = await db
      .select({
        content: memories.content,
        similarity,
        type: memories.type,
      })
      .from(memories)
      .where(
        and(
          eq(memories.userId, userId),
          gt(similarity, minSimilarity)
        )
      )
      .orderBy(desc(similarity))
      .limit(limit);

    return results;
  } catch (err) {
    // If OpenAI API key is missing or embedding fails, return empty
    // Chat still works without RAG context
    console.error("Context retrieval failed (embeddings may be unavailable):", err);
    return [];
  }
}
```

Key points:
- Uses `cosineDistance` from drizzle-orm for similarity calculation
- Filters by userId for data isolation
- Returns empty array on failure (graceful degradation if no OpenAI key)
- minSimilarity threshold of 0.3 (lower than research's 0.5 to avoid missing relevant context early on)

**2. Create `apps/server/src/memory/fact-extractor.ts`:**

Background fact extraction using a cheap/fast model in the onFinish callback:

```typescript
import { generateText, Output } from "ai";
import { UIMessage } from "ai";
import { eq, and } from "drizzle-orm";
import { z } from "zod";
import { db } from "../db";
import { userFacts } from "../db/schema";
import { getModel } from "../llm/providers";
import { embedText } from "./embeddings";

const factSchema = z.object({
  facts: z.array(
    z.object({
      category: z.enum(["goal", "preference", "context", "relationship", "work", "personal"]),
      fact: z.string(),
      confidence: z.number().min(0).max(1),
    })
  ),
});

export async function extractFacts(
  conversationId: string,
  userId: string,
  messages: UIMessage[]
): Promise<void> {
  // Only extract from the last exchange (last user + assistant messages)
  const recentMessages = messages.slice(-4);
  if (recentMessages.length === 0) return;

  const conversationText = recentMessages
    .map((m) => `${m.role}: ${typeof m.content === "string" ? m.content : JSON.stringify(m.content)}`)
    .join("\n");

  try {
    // Use a fast/cheap model for extraction
    // Try haiku first, fall back to the default model
    let modelId = "anthropic:claude-haiku-3-20250414";
    try {
      getModel(modelId);
    } catch {
      modelId = "anthropic:claude-sonnet-4-20250514";
    }

    const result = await generateText({
      model: getModel(modelId),
      system: `Extract key facts about the user from this conversation excerpt.
Only extract facts that are clearly stated or strongly implied by the user.
Do NOT extract facts about the assistant or general knowledge.
Return a JSON object with a "facts" array. Each fact should have:
- category: one of "goal", "preference", "context", "relationship", "work", "personal"
- fact: a concise statement about the user
- confidence: 0.0 to 1.0 (how confident you are this is a real fact about the user)

If no clear facts are present, return {"facts": []}.`,
      prompt: conversationText,
      output: Output.object({ schema: factSchema }),
    });

    const extracted = result.object;
    if (!extracted?.facts || extracted.facts.length === 0) return;

    // Store each fact with embedding
    for (const fact of extracted.facts) {
      // Check for duplicate or very similar existing facts
      const existing = await db
        .select()
        .from(userFacts)
        .where(
          and(
            eq(userFacts.userId, userId),
            eq(userFacts.category, fact.category),
            eq(userFacts.fact, fact.fact)
          )
        );

      if (existing.length > 0) continue; // Skip exact duplicates

      let embedding: number[] | null = null;
      try {
        embedding = await embedText(fact.fact);
      } catch {
        // Store without embedding if embedding fails
      }

      await db.insert(userFacts).values({
        userId,
        category: fact.category,
        fact: fact.fact,
        confidence: fact.confidence,
        source: "conversation",
        sourceId: conversationId,
        embedding,
      });
    }
  } catch (err) {
    // Don't let fact extraction failures affect the user experience
    console.error("Fact extraction error:", err);
  }
}
```

Key points:
- Uses a cheap model (Claude Haiku) to minimize cost and latency
- Only processes the last 4 messages (most recent exchange) to reduce token usage
- Deduplicates against existing facts by exact match
- Gracefully handles missing API keys (embedding or LLM)
- Fire-and-forget from onFinish — errors are caught and logged, never propagated
- Uses AI SDK v6 `Output.object()` for structured extraction
  </action>
  <verify>
1. `cd apps/server && npx tsc --noEmit` — no type errors
2. Verify retriever.ts exports retrieveContext
3. Verify fact-extractor.ts exports extractFacts
4. Both handle error cases gracefully (return empty / catch errors)
  </verify>
  <done>Semantic retriever queries memories table using pgvector cosine distance, filtered by userId. Fact extractor uses cheap LLM to extract user facts from conversations and stores them with embeddings. Both degrade gracefully when API keys are missing.</done>
</task>

</tasks>

<verification>
1. `cd apps/server && npx tsc --noEmit` passes
2. POST /api/chat route is mounted in index.ts with CORS
3. streamText + toUIMessageStreamResponse produces SSE stream
4. System prompt includes persona, mode, user facts, and RAG context
5. Mode detector returns correct mode for coaching and task messages
6. Retriever queries memories with cosine distance
7. Fact extractor runs in background after response completes
8. All error paths handled gracefully
</verification>

<success_criteria>
- POST /api/chat endpoint streams responses via SSE compatible with useChat
- System prompt dynamically includes RAG context and user facts
- Mode auto-detection distinguishes coaching vs task messages
- Manual mode override honored when set
- Conversation messages persisted after each response
- Facts extracted from conversations and stored in userFacts table
- Everything degrades gracefully when API keys are missing
</success_criteria>

<output>
After completion, create `.planning/phases/02-coaching-memory/02-02-SUMMARY.md`
</output>

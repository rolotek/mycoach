---
phase: 02-coaching-memory
plan: "03"
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - apps/server/src/documents/parser.ts
  - apps/server/src/documents/upload-route.ts
  - apps/server/src/trpc/router.ts
  - apps/server/src/index.ts
  - apps/server/package.json
autonomous: true

must_haves:
  truths:
    - "User can upload PDF, DOCX, and TXT files via POST /api/documents/upload"
    - "Uploaded documents are parsed, chunked, embedded, and stored in memories table"
    - "tRPC provides CRUD for conversations (list, get, delete)"
    - "tRPC provides CRUD for user facts (list, update, delete)"
    - "tRPC provides CRUD for documents (list, delete)"
    - "All tRPC queries filter by userId for data isolation"
  artifacts:
    - path: "apps/server/src/documents/parser.ts"
      provides: "PDF, DOCX, and TXT text extraction"
      exports: ["parseDocument"]
    - path: "apps/server/src/documents/upload-route.ts"
      provides: "Hono POST /api/documents/upload with multipart handling"
      exports: ["documentsApp"]
    - path: "apps/server/src/trpc/router.ts"
      provides: "Extended tRPC router with conversation, document, and userFact routers"
      exports: ["appRouter"]
  key_links:
    - from: "apps/server/src/documents/upload-route.ts"
      to: "apps/server/src/documents/parser.ts"
      via: "parseDocument call on uploaded file"
      pattern: "parseDocument"
    - from: "apps/server/src/documents/upload-route.ts"
      to: "apps/server/src/memory/embeddings.ts"
      via: "embedTexts for document chunks"
      pattern: "embedTexts"
    - from: "apps/server/src/documents/upload-route.ts"
      to: "apps/server/src/memory/chunker.ts"
      via: "chunkText for document splitting"
      pattern: "chunkText"
    - from: "apps/server/src/index.ts"
      to: "apps/server/src/documents/upload-route.ts"
      via: "app.route mounting"
      pattern: "documentsApp"
---

<objective>
Build document upload/processing pipeline and extend tRPC router with conversation, document, and user fact CRUD operations.

Purpose: Enables document knowledge (MEM-02) by parsing, chunking, and embedding uploaded files into the memories table for RAG retrieval. Provides tRPC endpoints for managing conversations, viewing/editing facts (MEM-04), and managing documents.
Output: Document parser, upload route, extended tRPC router.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-coaching-memory/02-RESEARCH.md
@.planning/phases/02-coaching-memory/02-01-SUMMARY.md

@apps/server/src/index.ts
@apps/server/src/db/schema.ts
@apps/server/src/trpc/router.ts
@apps/server/src/trpc/context.ts
@apps/server/src/memory/embeddings.ts
@apps/server/src/memory/chunker.ts
@apps/server/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document parser and upload route</name>
  <files>
    apps/server/src/documents/parser.ts
    apps/server/src/documents/upload-route.ts
    apps/server/src/index.ts
    apps/server/package.json
  </files>
  <action>
**1. Install document parsing dependencies:**

Add `unpdf` and `mammoth` to apps/server/package.json dependencies:
```bash
cd apps/server && npm install unpdf mammoth
```

**2. Create `apps/server/src/documents/parser.ts`:**

```typescript
import { extractText } from "unpdf";
import mammoth from "mammoth";

const SUPPORTED_TYPES = [
  "application/pdf",
  "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
  "text/plain",
] as const;

export type SupportedMimeType = (typeof SUPPORTED_TYPES)[number];

export function isSupportedType(mimeType: string): mimeType is SupportedMimeType {
  return SUPPORTED_TYPES.includes(mimeType as SupportedMimeType);
}

export async function parseDocument(
  buffer: Buffer,
  mimeType: string
): Promise<string> {
  switch (mimeType) {
    case "application/pdf": {
      const { text } = await extractText(new Uint8Array(buffer), {
        mergePages: true,
      });
      return text as string;
    }
    case "application/vnd.openxmlformats-officedocument.wordprocessingml.document": {
      const { value } = await mammoth.extractRawText({ buffer });
      return value;
    }
    case "text/plain":
      return buffer.toString("utf-8");
    default:
      throw new Error(`Unsupported file type: ${mimeType}`);
  }
}
```

**3. Create `apps/server/src/documents/upload-route.ts`:**

Create a Hono sub-app with POST /api/documents/upload:

```typescript
import { Hono } from "hono";
import { eq, and } from "drizzle-orm";
import { db } from "../db";
import { documents, memories } from "../db/schema";
import { parseDocument, isSupportedType } from "./parser";
import { chunkText } from "../memory/chunker";
import { embedTexts } from "../memory/embeddings";

const MAX_FILE_SIZE = 10 * 1024 * 1024; // 10MB

const documentsApp = new Hono<{
  Variables: {
    user: { id: string; name: string; email: string } | null;
    session: unknown;
  };
}>();

documentsApp.post("/api/documents/upload", async (c) => {
  const user = c.get("user");
  if (!user) return c.json({ error: "Unauthorized" }, 401);

  const body = await c.req.parseBody();
  const file = body["file"] as File | undefined;

  if (!file) return c.json({ error: "No file provided" }, 400);
  if (file.size > MAX_FILE_SIZE) {
    return c.json({ error: "File too large (max 10MB)" }, 400);
  }
  if (!isSupportedType(file.type)) {
    return c.json(
      { error: `Unsupported file type: ${file.type}. Supported: PDF, DOCX, TXT` },
      400
    );
  }

  // Create document record with processing status
  const [doc] = await db
    .insert(documents)
    .values({
      userId: user.id,
      filename: file.name,
      mimeType: file.type,
      size: file.size,
      status: "processing",
    })
    .returning();

  try {
    // Parse document text
    const buffer = Buffer.from(await file.arrayBuffer());
    const text = await parseDocument(buffer, file.type);

    // Update document with extracted text
    await db
      .update(documents)
      .set({ content: text })
      .where(eq(documents.id, doc.id));

    // Chunk text for embedding
    const chunks = chunkText(text, 500, 50);

    if (chunks.length > 0) {
      // Generate embeddings for all chunks
      let embeddings: number[][] | null = null;
      try {
        embeddings = await embedTexts(chunks);
      } catch (err) {
        console.error("Embedding generation failed for document:", err);
        // Continue without embeddings — document is still searchable by content
      }

      // Store chunks in memories table
      const memoryValues = chunks.map((chunk, i) => ({
        userId: user.id,
        content: chunk,
        embedding: embeddings?.[i] ?? null,
        type: "document_chunk" as const,
        metadata: {
          documentId: doc.id,
          filename: file.name,
          chunkIndex: i,
        },
      }));

      // Insert in batches of 50 to avoid query size limits
      for (let i = 0; i < memoryValues.length; i += 50) {
        await db.insert(memories).values(memoryValues.slice(i, i + 50));
      }
    }

    // Mark document as ready
    await db
      .update(documents)
      .set({ status: "ready" })
      .where(eq(documents.id, doc.id));

    return c.json({
      id: doc.id,
      filename: file.name,
      status: "ready",
      chunks: chunks.length,
    });
  } catch (err) {
    // Mark document as errored
    await db
      .update(documents)
      .set({ status: "error" })
      .where(eq(documents.id, doc.id));

    console.error("Document processing failed:", err);
    return c.json({ error: "Failed to process document" }, 500);
  }
});

export { documentsApp };
```

Key points:
- 10MB file size limit (Pitfall 7 from research)
- Validates file type before processing
- Creates document record first (so user sees it immediately), processes async-ish
- Chunks text and generates embeddings in batch
- Stores chunks in memories table with type "document_chunk" and metadata linking back to document
- Batches inserts to avoid query size limits
- Handles errors: marks document as "error" status if processing fails
- Gracefully handles missing OpenAI key (stores chunks without embeddings)

**4. Update `apps/server/src/index.ts`:**

- Import `documentsApp` from `./documents/upload-route`
- Add CORS for `/api/documents/*`: `app.use("/api/documents/*", cors(corsOptions))` (if not already added by Plan 02-02)
- Mount: `app.route("", documentsApp)` alongside the chatApp mounting
  </action>
  <verify>
1. `cd apps/server && npx tsc --noEmit` — no type errors
2. Verify parser.ts exports parseDocument and isSupportedType
3. Verify upload-route.ts exports documentsApp
4. Verify index.ts imports and mounts documentsApp with CORS
5. unpdf and mammoth are in package.json dependencies
  </verify>
  <done>Document upload endpoint accepts PDF/DOCX/TXT files up to 10MB, parses text, chunks with overlap, embeds via OpenAI, and stores chunks in memories table. Error handling marks documents as "error" on failure. CORS configured.</done>
</task>

<task type="auto">
  <name>Task 2: Extend tRPC router with conversation, document, and userFact CRUD</name>
  <files>apps/server/src/trpc/router.ts</files>
  <action>
Extend the existing tRPC router in router.ts. Keep the existing settingsRouter and llmRouter intact. Add three new routers:

**1. conversationRouter:**

- `list`: protectedProcedure, query. Returns all conversations for the authenticated user, ordered by updatedAt desc. Select id, title, mode, createdAt, updatedAt (do NOT return messages — too large for list view).

- `get`: protectedProcedure, input z.object({ id: z.string().uuid() }). Returns full conversation (including messages) for the authenticated user. Throws NOT_FOUND if not found or wrong user.

- `create`: protectedProcedure, input z.object({ mode: z.enum(["auto", "coaching", "task"]).default("auto") }). Creates a new conversation. Returns { id, mode }.

- `delete`: protectedProcedure, input z.object({ id: z.string().uuid() }). Deletes conversation and returns { success: true }. Filter by userId. Also delete associated memories (conversationId match) using a second delete query.

- `updateMode`: protectedProcedure, input z.object({ id: z.string().uuid(), mode: z.enum(["auto", "coaching", "task"]) }). Updates conversation mode. Filter by userId.

**2. documentRouter:**

- `list`: protectedProcedure, query. Returns all documents for the authenticated user, ordered by createdAt desc. Select id, filename, mimeType, size, status, createdAt.

- `delete`: protectedProcedure, input z.object({ id: z.string().uuid() }). Deletes the document and its associated memory chunks (where metadata->>'documentId' = doc.id OR where type = 'document_chunk' and metadata contains documentId). Filter by userId. Returns { success: true }.

**3. userFactRouter:**

- `list`: protectedProcedure, query. Returns all user facts for the authenticated user, ordered by category then createdAt desc. Select all fields except embedding.

- `update`: protectedProcedure, input z.object({ id: z.string().uuid(), fact: z.string().optional(), category: z.enum(["goal", "preference", "context", "relationship", "work", "personal"]).optional() }). Updates the specified fact. Filter by userId. Re-embed the fact if the text changed (import embedText from memory/embeddings). Returns updated fact.

- `delete`: protectedProcedure, input z.object({ id: z.string().uuid() }). Deletes the fact. Filter by userId. Returns { success: true }.

**Update the appRouter:**

```typescript
export const appRouter = t.router({
  settings: settingsRouter,
  llm: llmRouter,
  conversation: conversationRouter,
  document: documentRouter,
  userFact: userFactRouter,
});
```

Import needed schema tables at the top:
```typescript
import { userSettings, conversations, documents, userFacts, memories } from "../db/schema";
```

Import zod: `import { z } from "zod"` (already imported via updateSettingsSchema, but add explicit import).

Use `eq`, `and`, `desc`, `sql` from drizzle-orm as needed.

All queries MUST filter by ctx.user.id for data isolation. Never return data belonging to another user.

For the document delete, to delete associated memory chunks, use:
```typescript
await ctx.db.delete(memories).where(
  and(
    eq(memories.userId, ctx.user.id),
    eq(memories.type, "document_chunk"),
    sql`${memories.metadata}->>'documentId' = ${input.id}`
  )
);
```
  </action>
  <verify>
1. `cd apps/server && npx tsc --noEmit` — no type errors
2. Verify appRouter now includes conversation, document, and userFact routers
3. Each procedure filters by userId
4. Conversation delete also cleans up memories
5. Document delete also cleans up memory chunks
  </verify>
  <done>tRPC router extended with conversation (list, get, create, delete, updateMode), document (list, delete), and userFact (list, update, delete) routers. All queries filter by authenticated user's ID for data isolation. Cascade deletes clean up associated memories.</done>
</task>

</tasks>

<verification>
1. `cd apps/server && npx tsc --noEmit` passes
2. POST /api/documents/upload handles PDF, DOCX, TXT with size limits
3. Documents are parsed, chunked, embedded, and stored in memories
4. tRPC conversation.list returns conversations without messages
5. tRPC conversation.get returns full conversation with messages
6. tRPC document.list returns documents for the user
7. tRPC userFact.list returns facts for the user
8. All delete operations cascade to associated records
9. All operations filter by userId
</verification>

<success_criteria>
- Document upload works for PDF, DOCX, TXT up to 10MB
- Parsed text is chunked and embedded into memories table
- tRPC provides full CRUD for conversations, documents, and user facts
- All data access is isolated per user
- Document/conversation delete cascades to associated memories
</success_criteria>

<output>
After completion, create `.planning/phases/02-coaching-memory/02-03-SUMMARY.md`
</output>

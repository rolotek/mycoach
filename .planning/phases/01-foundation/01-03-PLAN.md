---
phase: 01-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - apps/server/src/llm/registry.ts
  - apps/server/src/llm/providers.ts
  - apps/server/package.json
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Provider registry can resolve model IDs from at least two providers"
    - "registry.languageModel('anthropic:claude-sonnet-4-20250514') returns a valid model object"
    - "registry.languageModel('ollama:llama3.1') returns a valid model object when Ollama is available"
    - "Provider SDKs are never imported directly in application code -- only the registry is used"
  artifacts:
    - path: "apps/server/src/llm/registry.ts"
      provides: "AI SDK provider registry with Anthropic, OpenAI, and Ollama"
      exports: ["registry"]
    - path: "apps/server/src/llm/providers.ts"
      provides: "Utility functions for LLM interaction using the registry"
      exports: ["chat", "getAvailableProviders"]
  key_links:
    - from: "apps/server/src/llm/registry.ts"
      to: "@ai-sdk/anthropic"
      via: "createProviderRegistry"
      pattern: "createProviderRegistry"
    - from: "apps/server/src/llm/providers.ts"
      to: "apps/server/src/llm/registry.ts"
      via: "registry.languageModel"
      pattern: "registry\\.languageModel"
---

<objective>
Create the AI SDK provider registry that enables provider-agnostic LLM calls. Register Anthropic, OpenAI, and Ollama providers. Expose utility functions that accept a model ID string and route to the correct provider. After this plan, any part of the server can make LLM calls by model ID string without importing provider SDKs directly.

Purpose: LLM-01 and LLM-02 require swappable backends. The registry pattern from AI SDK v6 is the standard solution -- register once, reference by string ID everywhere.
Output: Provider registry module and chat utility function.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: AI SDK provider registry with Anthropic, OpenAI, and Ollama</name>
  <files>
    apps/server/src/llm/registry.ts
    apps/server/package.json
  </files>
  <action>
    1. **Install AI SDK dependencies** in apps/server:
       ```bash
       cd apps/server && npm install ai @ai-sdk/anthropic @ai-sdk/openai ollama-ai-provider-v2
       ```

    2. **apps/server/src/llm/registry.ts**: Create provider registry following research Pattern 3 exactly:
       ```typescript
       import { createProviderRegistry } from "ai";
       import { anthropic } from "@ai-sdk/anthropic";
       import { openai } from "@ai-sdk/openai";
       import { createOllama } from "ollama-ai-provider-v2";

       const ollama = createOllama({
         baseURL: process.env.OLLAMA_BASE_URL || "http://localhost:11434/api",
       });

       export const registry = createProviderRegistry({
         anthropic,
         openai,
         ollama,
       });
       ```

       NOTE on ollama-ai-provider-v2: This is a recently published package (Feb 2026). If it fails to install or has incompatibility issues with AI SDK v6, fall back to using `@ai-sdk/openai` pointed at Ollama's OpenAI-compatible endpoint as documented in the research:
       ```typescript
       import { createOpenAI } from "@ai-sdk/openai";
       const ollama = createOpenAI({
         baseURL: process.env.OLLAMA_BASE_URL || "http://localhost:11434/v1",
         apiKey: "ollama", // required but unused
       });
       ```

    3. Verify the registry module can be imported without errors by adding a simple test in a temporary script or by importing it in the server entry and logging `typeof registry`.
  </action>
  <verify>
    - `cd apps/server && npx tsx -e "import { registry } from './src/llm/registry'; console.log('Registry loaded:', typeof registry)"` prints "Registry loaded: object"
    - No import or dependency resolution errors
  </verify>
  <done>
    Provider registry is created with Anthropic, OpenAI, and Ollama registered. Model IDs follow the `provider:model` format (e.g., "anthropic:claude-sonnet-4-20250514").
  </done>
</task>

<task type="auto">
  <name>Task 2: LLM utility functions for chat and provider listing</name>
  <files>
    apps/server/src/llm/providers.ts
  </files>
  <action>
    1. **apps/server/src/llm/providers.ts**: Create utility functions that wrap the registry:
       ```typescript
       import { generateText, streamText } from "ai";
       import { registry } from "./registry";

       // Resolve a model ID to a language model instance
       export function getModel(modelId: string) {
         return registry.languageModel(modelId);
       }

       // Non-streaming chat (for simple use cases)
       export async function chat(
         modelId: string,
         messages: Array<{ role: "user" | "assistant" | "system"; content: string }>,
       ) {
         const result = await generateText({
           model: getModel(modelId),
           messages,
         });
         return result;
       }

       // Streaming chat (for real-time UI)
       export function chatStream(
         modelId: string,
         messages: Array<{ role: "user" | "assistant" | "system"; content: string }>,
       ) {
         return streamText({
           model: getModel(modelId),
           messages,
         });
       }

       // List available providers and their common models
       // This is a static list -- dynamic model discovery is a future enhancement
       export function getAvailableProviders() {
         return [
           {
             id: "anthropic",
             name: "Anthropic (Claude)",
             requiresApiKey: true,
             envVar: "ANTHROPIC_API_KEY",
             models: [
               { id: "anthropic:claude-sonnet-4-20250514", name: "Claude Sonnet 4" },
               { id: "anthropic:claude-haiku-3-20250414", name: "Claude Haiku 3" },
             ],
           },
           {
             id: "openai",
             name: "OpenAI",
             requiresApiKey: true,
             envVar: "OPENAI_API_KEY",
             models: [
               { id: "openai:gpt-4o", name: "GPT-4o" },
               { id: "openai:gpt-4o-mini", name: "GPT-4o Mini" },
             ],
           },
           {
             id: "ollama",
             name: "Ollama (Local)",
             requiresApiKey: false,
             envVar: null,
             models: [
               { id: "ollama:llama3.1", name: "Llama 3.1" },
               { id: "ollama:mistral", name: "Mistral" },
             ],
           },
         ];
       }
       ```

    2. This module is the ONLY entry point for LLM operations. Application code (tRPC procedures, future agents) imports `chat`, `chatStream`, or `getModel` from this file -- never from `@ai-sdk/anthropic` or other provider packages directly.
  </action>
  <verify>
    - `cd apps/server && npx tsx -e "import { getAvailableProviders } from './src/llm/providers'; console.log(JSON.stringify(getAvailableProviders(), null, 2))"` prints the provider list
    - `cd apps/server && npx tsx -e "import { getModel } from './src/llm/providers'; const m = getModel('anthropic:claude-sonnet-4-20250514'); console.log('Model type:', m.modelId)"` prints the model ID (may fail if ANTHROPIC_API_KEY not set, but should not throw at resolution time -- only at call time)
  </verify>
  <done>
    LLM utility functions (`chat`, `chatStream`, `getModel`, `getAvailableProviders`) are implemented. All LLM operations go through the registry. No provider SDK is imported outside this module. Provider listing is available for the settings UI.
  </done>
</task>

</tasks>

<verification>
- Registry loads without errors
- getAvailableProviders returns 3 providers with models
- getModel resolves model IDs to language model objects
- No direct imports of @ai-sdk/anthropic or @ai-sdk/openai outside the llm/ directory
</verification>

<success_criteria>
- `createProviderRegistry` is initialized with anthropic, openai, and ollama
- `registry.languageModel("anthropic:claude-sonnet-4-20250514")` resolves successfully
- `getAvailableProviders()` returns structured provider/model list
- `chat()` and `chatStream()` accept any registered model ID string
- All LLM imports are encapsulated in apps/server/src/llm/
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
